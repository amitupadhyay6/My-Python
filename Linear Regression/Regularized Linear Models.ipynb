{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularized Linear Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw in Chapters 1 and 2, a good way to reduce overfitting is to regularize the\n",
    "model (i.e., to constrain it): the fewer degrees of freedom it has, the harder it will be\n",
    "for it to overfit the data. For example, a simple way to regularize a polynomial model\n",
    "is to reduce the number of polynomial degrees.\n",
    "For a linear model, regularization is typically achieved by constraining the weights of\n",
    "the model. We will now look at Ridge Regression, Lasso Regression, and Elastic Net,\n",
    "which implement three different ways to constrain the weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge Regression or Tikhonov regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge Regression (also called Tikhonov regularization) is a regularized version of Linear\n",
    "Regression: a regularization term equal to αΣi = 1\n",
    "n θi\n",
    "2 is added to the cost function.\n",
    "This forces the learning algorithm to not only fit the data but also keep the model\n",
    "weights as small as possible.\n",
    "\n",
    "Here is how to perform Ridge Regression with Scikit-Learn using a closed-form solution\n",
    "(a variant of Equation 4-9 using a matrix factorization technique by André-Louis\n",
    "Cholesky):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.linear_model import Ridge\n",
    "#ridge_reg = Ridge(alpha=1, solver=\"cholesky\")\n",
    "#ridge_reg.fit(X, y)\n",
    "#ridge_reg.predict([[1.5]])\n",
    "#      array([[1.55071465]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And using Stochastic Gradient Descent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sgd_reg = SGDRegressor(penalty=\"l2\")\n",
    "#sgd_reg.fit(X, y.ravel())\n",
    "#sgd_reg.predict([[1.5]])\n",
    "#     array([1.47012588])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The penalty hyperparameter sets the type of regularization term to use. Specifying\n",
    "\"l2\" indicates that you want SGD to add a regularization term to the cost function\n",
    "equal to half the square of the ℓ2 norm of the weight vector: this is simply Ridge\n",
    "Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lasso Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Least Absolute Shrinkage and Selection Operator Regression (simply called Lasso\n",
    "Regression) is another regularized version of Linear Regression: just like Ridge\n",
    "Regression, it adds a regularization term to the cost function, but it uses the ℓ1 norm\n",
    "of the weight vector instead of half the square of the ℓ2 norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important characteristic of Lasso Regression is that it tends to completely eliminate\n",
    "the weights of the least important features (i.e., set them to zero)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other words, Lasso Regression automatically performs feature selection and outputs a\n",
    "sparse model (i.e., with few nonzero feature weights)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here is a small Scikit-Learn example using the Lasso class. Note that you could\n",
    "#instead use an SGDRegressor(penalty=\"l1\").\n",
    "#from sklearn.linear_model import Lasso\n",
    "#lasso_reg = Lasso(alpha=0.1)\n",
    "#lasso_reg.fit(X, y)\n",
    "#lasso_reg.predict([[1.5]])\n",
    "#       array([1.53788174])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elastic Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elastic Net is a middle ground between Ridge Regression and Lasso Regression. The\n",
    "regularization term is a simple mix of both Ridge and Lasso’s regularization terms,\n",
    "and you can control the mix ratio r. When r = 0, Elastic Net is equivalent to Ridge\n",
    "Regression, and when r = 1, it is equivalent to Lasso Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So when should you use plain Linear Regression (i.e., without any regularization),\n",
    "Ridge, Lasso, or Elastic Net? It is almost always preferable to have at least a little bit of\n",
    "regularization, so generally you should avoid plain Linear Regression. Ridge is a good\n",
    "default, but if you suspect that only a few features are actually useful, you should prefer\n",
    "Lasso or Elastic Net since they tend to reduce the useless features’ weights down to\n",
    "zero as we have discussed. In general, Elastic Net is preferred over Lasso since Lasso\n",
    "may behave erratically when the number of features is greater than the number of\n",
    "training instances or when several features are strongly correlated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here is a short example using Scikit-Learn’s ElasticNet (l1_ratio corresponds to\n",
    "#the mix ratio r):\n",
    "#from sklearn.linear_model import ElasticNet\n",
    "#elastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5)\n",
    "#elastic_net.fit(X, y)\n",
    "#elastic_net.predict([[1.5]])\n",
    "#      array([1.54333232])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Early Stopping\n",
    "A very different way to regularize iterative learning algorithms such as Gradient\n",
    "Descent is to stop training as soon as the validation error reaches a minimum. This is\n",
    "called early stopping. Figure 4-20 shows a complex model (in this case a high-degree\n",
    "Polynomial Regression model) being trained using Batch Gradient Descent. As the\n",
    "epochs go by, the algorithm learns and its prediction error (RMSE) on the training set\n",
    "naturally goes down, and so does its prediction error on the validation set. However,\n",
    "\n",
    "after a while the validation error stops decreasing and actually starts to go back up.\n",
    "This indicates that the model has started to overfit the training data. With early stopping\n",
    "you just stop training as soon as the validation error reaches the minimum. It is\n",
    "such a simple and efficient regularization technique that Geoffrey Hinton called it a\n",
    "“beautiful free lunch.”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax Regression\n",
    "The Logistic Regression model can be generalized to support multiple classes directly,\n",
    "without having to train and combine multiple binary classifiers (as discussed in\n",
    "Chapter 3). This is called Softmax Regression, or Multinomial Logistic Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
